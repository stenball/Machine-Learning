{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Assignment 1 - ML&DM",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "nf4AVYLd9JtA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "43d437a6-0806-4869-9bfb-2a914de7e0ea"
      },
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"Assignment 1 - ML&DM\n",
        "\n",
        "Automatically generated by Colaboratory.\n",
        "\n",
        "Original file is located at\n",
        "    https://colab.research.google.com/drive/1TYATD6rqNwHFUUp48g1C08Z_fxje0c32\n",
        "\"\"\"\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import sys\n",
        "\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.model_selection import KFold, cross_val_score\n",
        "\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import BaggingClassifier\n",
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.svm import SVC\n",
        "\n",
        "dataPath = sys.argv[1]\n",
        "classifier = sys.argv[2]\n",
        "\n",
        "if len(sys.argv) > 3:\n",
        "  configPath = sys.argv[3]\n",
        "  config = pd.read_csv(configPath)\n",
        "\n",
        "df1 = pd.read_csv(dataPath)\n",
        "\n",
        "\n",
        "#classifier = 'NN'\n",
        "#config = pd.read_csv('/content/drive/My Drive/ML/config.csv')\n",
        "#df1 = pd.read_csv('/content/drive/My Drive/ML/breast-cancer-wisconsin.csv')\n",
        "\n",
        "#df2 = pd.read_csv('/content/drive/My Drive/ML/breast-cancer-wisconsin-normalised.csv')\n",
        "\n",
        "def getAccScore(model, X_train, X_test, y_train, y_test):\n",
        "    model.fit(X_train, y_train)\n",
        "    return model.score(X_test, y_test)\n",
        "\n",
        "def stratKFold(model, X, y):\n",
        "  kf = StratifiedKFold(n_splits=10, shuffle=True, random_state=0)\n",
        "  scores =[]\n",
        "\n",
        "  for train_index,test_index in kf.split(X, y):\n",
        "    xtr,xte = X.iloc[train_index],X.iloc[test_index]\n",
        "    ytr,yte = y.iloc[train_index],y.iloc[test_index]\n",
        "    \n",
        "    score = getAccScore(model, xtr, xte, ytr, yte)\n",
        "    scores.append(score)\n",
        "  \n",
        "  return scores\n",
        "\n",
        "def kNNClassifier(X, y, k=1):\n",
        "  knn = KNeighborsClassifier(n_neighbors=k)\n",
        "  scores = stratKFold(knn, X, y)\n",
        "  return scores, sum(scores)/len(scores)\n",
        "\n",
        "def logregClassifier(X, y):\n",
        "  logreg = LogisticRegression(random_state=0)\n",
        "  scores = stratKFold(logreg, X, y)\n",
        "  return scores, sum(scores)/len(scores)\n",
        "\n",
        "def nbClassifier(X, y):\n",
        "  nb = GaussianNB()\n",
        "  scores = stratKFold(nb, X, y)\n",
        "  return scores, sum(scores)/len(scores)\n",
        "\n",
        "def dtClassifier(X, y):\n",
        "  tree = DecisionTreeClassifier(criterion='entropy', random_state=0)\n",
        "  scores = stratKFold(tree, X, y)\n",
        "  return scores, sum(scores)/len(scores)\n",
        "\n",
        "def bagDTClassifier(X, y, n_estimators=10, max_samples=10, max_depth=10):\n",
        "  bag_clf = BaggingClassifier(DecisionTreeClassifier(random_state=0, max_depth=max_depth, criterion = 'entropy'), n_estimators=n_estimators, max_samples=max_samples, bootstrap=True, random_state=0)\n",
        "  scores = stratKFold(bag_clf, X, y)\n",
        "  return scores, sum(scores)/len(scores)\n",
        "\n",
        "def adaDTClassifier(X, y, n_estimators=10, learning_rate=0.01, max_depth=10):\n",
        "  ada_clf = AdaBoostClassifier(\n",
        "    DecisionTreeClassifier(random_state=0, max_depth=max_depth, criterion = 'entropy'), n_estimators=n_estimators, learning_rate=learning_rate, random_state=0)\n",
        "  scores = stratKFold(ada_clf, X, y)\n",
        "  return scores, sum(scores)/len(scores)\n",
        "\n",
        "def gbClassifier(X, y, n_estimators=10, learning_rate=0.01):\n",
        "  gb_clf = GradientBoostingClassifier(n_estimators=n_estimators, learning_rate=learning_rate, random_state=0)\n",
        "  scores = stratKFold(gb_clf, X, y)\n",
        "  return scores, sum(scores)/len(scores)\n",
        "\n",
        "\n",
        "\n",
        "def bestLinClassifier(X,y):\n",
        "  X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, stratify=y, random_state=0)\n",
        "\n",
        "  param_grid = [{'C': [0.001, 0.01, 0.1, 1, 10, 100],'gamma': [0.001, 0.01, 0.1, 1, 10, 100]}]\n",
        "\n",
        "  #skf = StratifiedKFold(n_splits=10, shuffle=True, random_state=0)\n",
        "  grid = GridSearchCV(SVC(kernel ='linear'),param_grid,refit=True,cv=StratifiedKFold(n_splits=10, shuffle=True, random_state=0), return_train_score=True)\n",
        "  grid.fit(X_train, y_train)\n",
        "  print(grid.best_params_.get('C'))\n",
        "  print(grid.best_params_.get('gamma'))\n",
        "\n",
        "  #scores = cross_val_score(grid.best_estimator_, X, y, cv = skf) \n",
        "\n",
        "  #scores =[]\n",
        "\n",
        "  #for train_index,test_index in skf.split(X, y):\n",
        "  #  xtr,xte = X.iloc[train_index],X.iloc[test_index]\n",
        "  #  ytr,yte = y.iloc[train_index],y.iloc[test_index]\n",
        "    \n",
        "  #  score = getAccScore(grid.best_estimator_, xtr, xte, ytr, yte)\n",
        "  #  scores.append(score)\n",
        "\n",
        "\n",
        "  #mean_score = sum(scores)/len(scores)\n",
        "  #print('%.4f' % mean_score.round(4))\n",
        "\n",
        "  #y_pred_rf = grid.best_estimator_.predict(X_test)\n",
        "  #scoretwo = accuracy_score(y_test, y_pred_rf)\n",
        "  #print('%.4f' % scoretwo.round(4))\n",
        "\n",
        "  print('%.4f' % grid.best_score_)\n",
        "  print('%.4f' % grid.score(X_test, y_test))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def bestRFClassifier(X,y):\n",
        "  X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, stratify=y, random_state=0)\n",
        "\n",
        "  param_grid = [{'n_estimators': [10, 20, 50, 100], \n",
        "                 'max_features': ['auto', 'sqrt', 'log2'],\n",
        "                 'max_leaf_nodes': [10, 20 ,30]}]\n",
        "\n",
        "  #skf = StratifiedKFold(n_splits=10, shuffle=True, random_state=0)\n",
        "  grid = GridSearchCV(RandomForestClassifier(criterion='entropy', random_state = 0),param_grid,cv=StratifiedKFold(n_splits=10, shuffle=True, random_state=0), return_train_score=True)\n",
        "  grid.fit(X_train, y_train)\n",
        "\n",
        "  print(grid.best_params_.get('n_estimators'))\n",
        "  print(grid.best_params_.get('max_features'))\n",
        "  print(grid.best_params_.get('max_leaf_nodes'))\n",
        "\n",
        "  #scores = cross_val_score(grid.best_estimator_, X, y, cv = skf) \n",
        "  #scores =[]\n",
        "\n",
        "  #for train_index,test_index in skf.split(X, y):\n",
        "  #  xtr,xte = X.iloc[train_index],X.iloc[test_index]\n",
        "  #  ytr,yte = y.iloc[train_index],y.iloc[test_index]\n",
        "    \n",
        "  #  score = getAccScore(grid.best_estimator_, xtr, xte, ytr, yte)\n",
        "  #  scores.append(score)\n",
        "\n",
        "  #mean_score = sum(scores)/len(scores)\n",
        "  #print('%.4f' % mean_score.round(4))\n",
        "  \n",
        "  print('%.4f' % grid.best_score_)\n",
        "  print('%.4f' % grid.score(X_test, y_test))\n",
        "\n",
        "  #y_pred_rf = grid.best_estimator_.predict(X_test)\n",
        "  #scoretwo = accuracy_score(y_test, y_pred_rf)\n",
        "  #print('%.4f' % scoretwo.round(4))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def pre_process(df):\n",
        "  df['class'] = df['class'].replace('class1','0')\n",
        "  df['class'] = df['class'].replace('class2','1')\n",
        "\n",
        "\n",
        "  df.replace('?',np.NaN,inplace=True)\n",
        "  imp=SimpleImputer(missing_values=np.NaN)\n",
        "  df_transformed=pd.DataFrame(imp.fit_transform(df))\n",
        "  df_transformed.columns=df.columns\n",
        "  df_transformed.index=df.index\n",
        "  df1 = df_transformed.copy()\n",
        "  scaler = MinMaxScaler()\n",
        "  scaled = scaler.fit_transform(df1)\n",
        "  df1 = pd.DataFrame(scaled, index=df1.index, columns=df1.columns) \n",
        "  df1 = df1.round(4)\n",
        "  \n",
        "  X = df1.iloc[:, df1.columns != 'class']\n",
        "  y = df1.iloc[:,-1]\n",
        "  \n",
        "  inde_x = 0\n",
        "  for index, row in X.iterrows():\n",
        "    for (columnName, columnData) in X.iteritems():\n",
        "      print(\"%.4f\" % row[columnName], end = '')\n",
        "      print(',', end = '')\n",
        "    print(\"%.0f\" % y.iloc[inde_x])\n",
        "    inde_x = inde_x + 1\n",
        "\n",
        "  print('', end='')\n",
        "  return df1\n",
        "\n",
        "def split_data(df1):\n",
        "  X = df1.iloc[:, df1.columns != 'class']\n",
        "  y = df1.iloc[:,-1]\n",
        "\n",
        "  X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, stratify=y, random_state=0)\n",
        "  \n",
        "  return X, y, X_train, X_test, y_train, y_test\n",
        "\n",
        "mean_score = None\n",
        "to_round = 0\n",
        "#X, y, X_train, X_test, y_train, y_test = split_data(df1)\n",
        "\n",
        "X = df1.iloc[:, df1.columns != 'class']\n",
        "y = df1.iloc[:,-1]\n",
        "\n",
        "if classifier =='NN':\n",
        "  if len(sys.argv) > 3:\n",
        "    scores, mean_score = kNNClassifier(X, y, config['K'].iloc[0])\n",
        "  else:\n",
        "    scores, mean_score = kNNClassifier(X, y)\n",
        "\n",
        "elif classifier == 'LR':\n",
        "   scores, mean_score = logregClassifier(X, y)\n",
        "\n",
        "elif classifier == 'NB':\n",
        "   scores, mean_score = nbClassifier(X, y)\n",
        "\n",
        "elif classifier == 'DT':\n",
        "   scores, mean_score = dtClassifier(X, y)\n",
        "\n",
        "elif classifier == 'BAG':\n",
        "  if len(sys.argv) > 3:\n",
        "    scores, mean_score = bagDTClassifier(X, y, config['n_estimators'].iloc[0], config['max_samples'].iloc[0], config['max_depth'].iloc[0])\n",
        "  else:\n",
        "    scores, mean_score = bagDTClassifier(X, y)\n",
        "\n",
        "elif classifier == 'ADA':\n",
        "  if len(sys.argv) > 3:\n",
        "    scores, mean_score = adaDTClassifier(X, y, config['n_estimators'].iloc[0], config['learning_rate'].iloc[0], config['max_depth'].iloc[0])\n",
        "  else:\n",
        "    scores, mean_score = adaDTClassifier(X, y)\n",
        "\n",
        "elif classifier == 'GB':\n",
        "  if len(sys.argv) > 3:\n",
        "    scores, mean_score = gbClassifier(X, y, config['n_estimators'].iloc[0], config['learning_rate'].iloc[0])\n",
        "  else:\n",
        "    scores, mean_score = gbClassifier(X, y)\n",
        "\n",
        "elif classifier == 'RF':\n",
        "   bestRFClassifier(X, y)\n",
        "\n",
        "elif classifier == 'SVM':\n",
        "   bestLinClassifier(X, y)\n",
        "\n",
        "elif classifier == 'P':\n",
        "   df = pre_process(df1)\n",
        "\n",
        "else:\n",
        "   print('Enter a valid classifier')\n",
        "   to_round = 1\n",
        "\n",
        "if to_round == 0 and classifier != 'P' and classifier != 'RF' and classifier != 'SVM' and classifier != 'RBF':\n",
        "  print(\"%.4f\" % mean_score.round(4))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "20\n",
            "auto\n",
            "30\n",
            "0.9628\n",
            "0.9943\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}